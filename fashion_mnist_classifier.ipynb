{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe data features can be extracted as train_dataset/test_dataset.data\\nThe labels can be extracted as train_dataset/test_dataset.targets \\n\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Download and prepare the Fashion MNIST data set \n",
    "train_dataset = dsets.FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = dsets.FashionMNIST(root='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride = 1, padding=0)\n",
    "        self.fc1   = nn.Linear(in_features=6*12*12, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.softmax(self.fc1(x),dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % (len(train_loader)//2) == 0:\n",
    "            print('Train({})[{:.0f}%]: Loss: {:.4f}'.format(\n",
    "                epoch, 100. * batch_idx / len(train_loader), train_loss/(batch_idx+1)))\n",
    "\n",
    "def test(model, device, test_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss = (test_loss*batch_size)/len(test_loader.dataset)\n",
    "    print('Test({}): Loss: {:.4f}, Accuracy: {:.4f}%'.format(\n",
    "        epoch, test_loss, 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 2.3032\n",
      "Train(1)[50%]: Loss: 1.7359\n",
      "Train(1)[100%]: Loss: 1.6970\n",
      "Test(1): Loss: 1.6577, Accuracy: 81.2200%\n",
      "Optimizer Learning rate: 0.0010\n",
      "Train(2)[0%]: Loss: 1.5691\n",
      "Train(2)[50%]: Loss: 1.6401\n",
      "Train(2)[100%]: Loss: 1.6320\n",
      "Test(2): Loss: 1.6343, Accuracy: 83.4400%\n",
      "Optimizer Learning rate: 0.0010\n",
      "Train(3)[0%]: Loss: 1.5791\n",
      "Train(3)[50%]: Loss: 1.6180\n",
      "Train(3)[100%]: Loss: 1.6166\n",
      "Test(3): Loss: 1.6246, Accuracy: 84.2800%\n",
      "Optimizer Learning rate: 0.0010\n",
      "Train(4)[0%]: Loss: 1.6791\n",
      "Train(4)[50%]: Loss: 1.6090\n",
      "Train(4)[100%]: Loss: 1.6074\n",
      "Test(4): Loss: 1.6169, Accuracy: 84.9500%\n",
      "Optimizer Learning rate: 0.0010\n",
      "Train(5)[0%]: Loss: 1.6482\n",
      "Train(5)[50%]: Loss: 1.6039\n",
      "Train(5)[100%]: Loss: 1.6008\n",
      "Test(5): Loss: 1.6119, Accuracy: 85.7100%\n",
      "Optimizer Learning rate: 0.0001\n",
      "Train(6)[0%]: Loss: 1.5695\n",
      "Train(6)[50%]: Loss: 1.5906\n",
      "Train(6)[100%]: Loss: 1.5911\n",
      "Test(6): Loss: 1.6036, Accuracy: 86.3900%\n",
      "Optimizer Learning rate: 0.0001\n",
      "Train(7)[0%]: Loss: 1.6416\n",
      "Train(7)[50%]: Loss: 1.5880\n",
      "Train(7)[100%]: Loss: 1.5899\n",
      "Test(7): Loss: 1.6032, Accuracy: 86.4000%\n",
      "Optimizer Learning rate: 0.0001\n",
      "Train(8)[0%]: Loss: 1.4771\n",
      "Train(8)[50%]: Loss: 1.5899\n",
      "Train(8)[100%]: Loss: 1.5891\n",
      "Test(8): Loss: 1.6028, Accuracy: 86.3800%\n",
      "Optimizer Learning rate: 0.0001\n",
      "Train(9)[0%]: Loss: 1.6174\n",
      "Train(9)[50%]: Loss: 1.5888\n",
      "Train(9)[100%]: Loss: 1.5883\n",
      "Test(9): Loss: 1.6030, Accuracy: 86.4200%\n",
      "Optimizer Learning rate: 0.0001\n",
      "Train(10)[0%]: Loss: 1.5553\n",
      "Train(10)[50%]: Loss: 1.5889\n",
      "Train(10)[100%]: Loss: 1.5877\n",
      "Test(10): Loss: 1.6017, Accuracy: 86.5400%\n",
      "Optimizer Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model = Model()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer =  optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "scheduler = scheduler = optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5],gamma=0.1)\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, device, test_loader, criterion, epoch)\n",
    "    scheduler.step()\n",
    "    print('Optimizer Learning rate: {0:.4f}'.format(optimizer.param_groups[0]['lr']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
